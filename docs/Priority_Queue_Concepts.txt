
When the scraper is recursively following links:

* **Depth 0** → the page the user initally requested → user is waiting → highest priority.
* **Depth 1, 2, ...** → pages discovered later → user isn't directly waiting → lower priority.

| depth |             priority |
| ----: | -------------------: |
|     0 | 0 (highest priority) |
|     1 |                    1 |
|     2 |                    2 |
|   ... |                    n |

New requests (depth=0) always jump ahead of deep recursions → faster feedback for the user.


We can easily set the event priority by assigning our Depth variable which represetns the link depth of recursive crawls.
This is a natural fit as depth 0 is the root page request, and subsequent Depth are the followed links.

* The initial call (depth=0) uses priority=0
* Recursive calls increment `depth`, which also increases `priority`.

Since lower numbers mean higher priority, this naturally works.

* `depth=0 → priority=0 → highest`
* `depth=1 → priority=1 → lower`
* `depth=2 → priority=2 → even lower`

This way:

* User-triggered requests always jump to the front.
* Deeper recursions get processed behind shallow recursions.
* Your user sees the page they clicked on faster, even if deep pages are still being scraped.
