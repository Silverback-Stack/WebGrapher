
## 🧭 **Scenario:**

User submits URL `B` to crawl → HTTP response redirects to `C`


User submits request to crawl URL: B
└─> Crawl result: IsRedirect = true
          RequestUrl = B
          ResolvedUrl = C
          Links in page: [ ... ] ← only applies if page was actually fetched;
                                       for a redirect, usually empty


Step 1: Handle the redirect node
--------------------------------
Check if Node B exists
 ├─ If missing: create Node B
 └─ Mark Node B:
     State: Redirected
     RedirectsTo: C
     + Add redirect edge: B --(FromRedirect)--> C

Persist Node B


Step 2: Handle resolved node (target of redirect)
-------------------------------------------------
Check if Node C exists
 ├─ If missing: create Node C as Populated
 |    + Set title, keywords, source last modified
 |    + Add edges: C → X, C → Y, etc.
 |    + Persist Node C
 |    + FollowEdges(C): schedule crawls for X, Y, etc.
 ├─ If Node C exists but is Dummy:
 |    + Populate it as above
 |    + Persist Node C
 |    + FollowEdges(C)
 └─ If Node C is Populated and stale:
      + Refresh content & edges
      + Persist Node C
      + FollowEdges(C)

Else if Node C is Populated and fresh: do nothing



Graph nodes:
─────────────
Node B:
    - State: Redirected
    - RedirectsTo: C
    - Edges:
        B --(FromRedirect, RedirectedFrom=B)--> C

Node C:
    - State: Populated
    - Title, keywords, lastModified, etc.
    - Edges:
        C --(FromSource)--> X
        C --(FromSource)--> Y
        ...


Then

* `FollowEdges` walks the edges out of `C` (the resolved, populated node).
* For each target node (e.g., X, Y, …):

  * If missing or dummy → schedule crawl.
  * If already populated and fresh → skip.

